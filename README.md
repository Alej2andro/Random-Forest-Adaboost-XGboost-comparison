# üß¨ Ensemble Algorithms: Random Forest, AdaBoost & XGBoost
### Detecci√≥n de C√°ncer de Mama - An√°lisis Comparativo con Desbalanceo de Clases

[![üìÑ Documentaci√≥n Completa](https://img.shields.io/badge/Documentaci√≥n-GitHub%20Pages-2ea44f?style=for-the-badge)](https://alej2andro.github.io/Random-Forest-Adaboost-XGboost-comparison/)
[![üìä Blog RPubs](https://img.shields.io/badge/Blog-RPubs-ff6600?style=for-the-badge&logo=r)](https://rpubs.com/Alej5ndro)

---

## üéØ Sobre Este Proyecto

An√°lisis matem√°tico riguroso de tres algoritmos de ensamble aplicados al dataset **Wisconsin Breast Cancer**, explorando c√≥mo diferentes t√©cnicas manejan desbalanceo de clases y optimizan el trade-off cr√≠tico entre sensitivity y specificity en diagn√≥stico cl√≠nico.

**Motivaci√≥n**: Como autodidacta apasionado por las matem√°ticas, la ciencia de datos y el machine learning, este proyecto profundiza en los fundamentos te√≥ricos y aplicaciones pr√°cticas de algoritmos de ensamble, demostrando c√≥mo la teor√≠a estad√≠stica se traduce en soluciones reales de IA.

## üìê Fundamentos Matem√°ticos Implementados

### Random Forest (Bagging)
- Reducci√≥n de varianza mediante bootstrap aggregation
- Decorrelaci√≥n de √°rboles con selecci√≥n aleatoria de features ($m = \lfloor\sqrt{p}\rfloor$)
- Ecuaci√≥n de varianza: $\text{Var}(\hat{f}_{rf}) = \rho\sigma^2 + \frac{1-\rho}{B}\sigma^2$

### AdaBoost (Boosting Adaptativo)
- Minimizaci√≥n de p√©rdida exponencial: $L(y, f(x)) = \exp(-yf(x))$
- Pesos adaptativos: $\alpha_m = \log\left(\frac{1 - \text{err}_m}{\text{err}_m}\right)$
- Correcci√≥n secuencial de errores mediante reasignaci√≥n de pesos

### XGBoost (Gradient Boosting con Regularizaci√≥n)
- Aproximaci√≥n de Taylor de segundo orden (gradiente + Hessiano)
- Regularizaci√≥n expl√≠cita: $\Omega(f) = \gamma T + \frac{1}{2}\lambda\sum_{j=1}^T w_j^2$
- Optimizaci√≥n de ganancia por split con penalizaci√≥n de complejidad

## üìä Resultados & M√©tricas

| Algoritmo | Accuracy | Sensitivity | F1-Score | AUC-ROC | Generalizaci√≥n |
|-----------|----------|-------------|----------|---------|----------------|
| **Random Forest** | 0.9569 | 0.9444 | 0.9379 | 0.9919 | ‚≠ê **Mejor** (-0.010) |
| **AdaBoost** | 0.9569 | 0.9583 | 0.9388 | 0.9901 | Buena (-0.016) |
| **XGBoost** | **0.9665** | **0.9861** | **0.9530** | **0.9934** | Moderada (-0.030) |

### üîç Hallazgos Clave

**XGBoost** ‚Üí Mejor rendimiento en test (98.6% sensitivity, solo 1 FN)  
**Random Forest** ‚Üí Superior generalizaci√≥n en datos nuevos (F1=0.984)  
**AdaBoost** ‚Üí M√°xima detecci√≥n en screening (99.5% sensitivity)

### üí° Insights de Machine Learning

1. **Trade-off Optimizaci√≥n vs Generalizaci√≥n**: XGBoost maximiza m√©tricas en test pero Random Forest mantiene estabilidad en producci√≥n
2. **Manejo de Desbalanceo**: Class weights (RF/XGBoost) y pesos adaptativos (AdaBoost) logran sensitivity >94% sin sacrificar specificity
3. **Importancia de Features**: Consistencia cross-algoritmo en variables nucleares (Bare.nuclei, Cell.shape) valida relevancia cl√≠nica

## üõ†Ô∏è Stack T√©cnico

**Lenguaje**: R 4.x  
**ML Frameworks**: `randomForest`, `adabag`, `xgboost`  
**Evaluaci√≥n**: `caret`, `pROC`, `PRROC`  
**Visualizaci√≥n**: `ggplot2` (curvas ROC, matrices de confusi√≥n, fronteras de decisi√≥n)  
**Documentaci√≥n**: Quarto + GitHub Pages

## üöÄ Reproducibilidad
```r
# Instalar dependencias
install.packages(c("mlbench", "randomForest", "xgboost", "caret", 
                   "pROC", "PRROC", "ggplot2", "adabag", "reshape2"))

# Renderizar an√°lisis
quarto::quarto_render("Randomforest_adaboost_xgboost.qmd")
```

## üìö Fundamentos Te√≥ricos Aplicados

- **Teor√≠a de Ensambles**: Sesgo-Varianza, Decorrelaci√≥n de Predictores
- **Optimizaci√≥n Num√©rica**: Descenso de Gradiente, Aproximaci√≥n de Taylor
- **Teor√≠a de la Informaci√≥n**: Impureza de Gini, Ganancia de Informaci√≥n
- **Estad√≠stica Bayesiana**: Pesos posteriores en clasificadores secuenciales

## üéì Contexto de Aprendizaje

Este proyecto representa mi camino autodidacta en **ciencia de datos, machine learning e inteligencia artificial**, donde las matem√°ticas son el lenguaje fundamental para entender algoritmos m√°s all√° de librer√≠as black-box. Cada l√≠nea de c√≥digo refleja comprensi√≥n te√≥rica traducida a implementaci√≥n pr√°ctica.

**√Åreas de Especializaci√≥n**:
- Fundamentos matem√°ticos de ML (√°lgebra lineal, c√°lculo, probabilidad)
- Algoritmos de ensamble y optimizaci√≥n
- Evaluaci√≥n de modelos en contextos de alta criticidad (medicina, finanzas)
- Visualizaci√≥n de decisiones algor√≠tmicas

## üìñ Referencias

- Breiman, L. (2001). *Random Forests*. Machine Learning, 45(1), 5-32
- Freund & Schapire (1997). *A Decision-Theoretic Generalization of On-Line Learning*
- Chen & Guestrin (2016). *XGBoost: A Scalable Tree Boosting System*
- Hastie et al. (2009). *The Elements of Statistical Learning*

---

## üë®‚Äçüíª Autor

**Alejandro Figueroa Rojas**  
*Apasionado autodidacta en Matem√°ticas, Ciencia de Datos, Machine Learning e IA*

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=flat&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/alejandrofigueroarojas)
[![Email](https://img.shields.io/badge/Email-D14836?style=flat&logo=gmail&logoColor=white)](mailto:alejandro.figueroa.rojas@gmail.com)
[![RPubs](https://img.shields.io/badge/RPubs-75AADB?style=flat&logo=r&logoColor=white)](https://rpubs.com/Alej5ndro)

---

‚≠ê **Si este proyecto te resulta √∫til o inspira tu aprendizaje en ML/IA, considera darle una estrella**
